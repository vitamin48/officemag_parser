"""
–®–∞–≥ 3. –°–±–æ—Ä –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–≤–∞—Ä–∞—Ö (—Ñ–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å —Ü–≤–µ—Ç–Ω—ã–º–∏ –ª–æ–≥–∞–º–∏).
"""
import os
import time
import datetime
import json
import random
import re
import requests
import platform
import socket
from playwright.sync_api import sync_playwright, TimeoutError, Page
from tqdm import tqdm
from colorama import init, Fore, Style

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–µ–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏—Ç—Å—è, —Å–∫—Ä–∏–ø—Ç –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π.
try:
    from config import BOT_TOKEN, CHAT_ID
except ImportError:
    BOT_TOKEN, CHAT_ID = None, None

# --- –ù–ê–°–¢–†–û–ô–ö–ò –°–ö–†–ò–ü–¢–ê ---
INPUT_URL_FILE = os.path.join("in", "product_links.txt")
OUTPUT_JSON_FILE = os.path.join("out", "products_data.json")
OUTPUT_FAILED_FILE = os.path.join("out", "failed_urls.txt")
DEBUG_DIR = os.path.join("out", "debug")

LOGIN_URL = "https://www.officemag.ru/auth/"
USER_LOGIN = "forvk180420@gmail.com"
USER_PASSWORD = "forvk180420"

BASE_URL = "https://www.officemag.ru"

HEADLESS_MODE = False
TIMEOUT = 45000
MAX_RETRIES = 3
PAUSE_BETWEEN_REQUESTS = (2, 5)


# --- –§–£–ù–ö–¶–ò–ò ---

def send_logs_to_telegram(message: str):
    BOT_TOKEN = '6456958617:AAEhKxpvbWxeDoq7IPf7fQo0sxbQ_LqSVz0'
    CHAT_ID = '128592002'
    try:
        platform_info = platform.system()
        hostname = socket.gethostname()
        user = os.getlogin()
        full_message = message + f'\n\n---\nüñ•Ô∏è {platform_info}\nüë§ {hostname}\\{user}'
        url = f'https://api.telegram.org/bot{BOT_TOKEN}/sendMessage'
        data = {"chat_id": CHAT_ID, "text": full_message}
        requests.post(url, data=data, timeout=10)
    except Exception as e:
        print(Fore.RED + f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –≤ Telegram: {e}")


def save_debug_info(page: Page, article_id: str):
    print(Fore.MAGENTA + f"!!! –°–æ—Ö—Ä–∞–Ω—è—é –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –∞—Ä—Ç–∏–∫—É–ª–∞ {article_id}...")
    os.makedirs(DEBUG_DIR, exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    screenshot_path = os.path.join(DEBUG_DIR, f"{article_id}_{timestamp}_debug.png")
    html_path = os.path.join(DEBUG_DIR, f"{article_id}_{timestamp}_debug.html")
    try:
        page.screenshot(path=screenshot_path, full_page=True)
        print(Fore.MAGENTA + f"  - –°–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {screenshot_path}")
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(page.content())
        print(Fore.MAGENTA + f"  - HTML-–∫–æ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {html_path}")
    except Exception as e:
        print(Fore.RED + f"  - –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é: {e}")


def read_simple_urls(filepath: str) -> list[str]:
    if not os.path.exists(filepath):
        print(Fore.YELLOW + f"–û–®–ò–ë–ö–ê: –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
        return []
    with open(filepath, 'r', encoding='utf-8') as f:
        urls = [line.strip() for line in f if line.strip().startswith('http')]
    unique_urls = list(dict.fromkeys(urls))
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {Fore.GREEN}{len(unique_urls)}{Style.RESET_ALL} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑ {filepath}.")
    return unique_urls


def load_existing_data(filepath: str) -> dict:
    if not os.path.exists(filepath): return {}
    with open(filepath, 'r', encoding='utf-8') as f:
        try:
            data = json.load(f)
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {Fore.GREEN}{len(data)}{Style.RESET_ALL} —É–∂–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ JSON.")
            return data
        except json.JSONDecodeError:
            print(Fore.YELLOW + f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: JSON-—Ñ–∞–π–ª {filepath} –ø–æ–≤—Ä–µ–∂–¥–µ–Ω. –ù–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è.")
            return {}


def save_json_data(data: dict, filepath: str):
    output_dir = os.path.dirname(filepath)
    os.makedirs(output_dir, exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


def log_failed_url(url: str, reason: str, filepath: str):
    output_dir = os.path.dirname(filepath)
    os.makedirs(output_dir, exist_ok=True)
    with open(filepath, 'a', encoding='utf-8') as f:
        f.write(f"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M')} | {reason} | {url}\n")


def perform_login(page: Page):
    print("–í—ã–ø–æ–ª–Ω—è—é –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é...")
    try:
        page.goto(LOGIN_URL)
        page.wait_for_load_state('domcontentloaded')
        time.sleep(3)
        page.keyboard.press("Escape")
        time.sleep(1)
        page.get_by_label("–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –ø–æ—á—Ç–∞ –∏–ª–∏ –ª–æ–≥–∏–Ω").fill(USER_LOGIN)
        page.get_by_label("–ü–∞—Ä–æ–ª—å").fill(USER_PASSWORD)
        page.get_by_role("button", name="–í–æ–π—Ç–∏").click()
        page.wait_for_selector("span.User__trigger:has-text('–ö–∞–±–∏–Ω–µ—Ç')", timeout=15000)
        print(Fore.GREEN + "–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ.")
        return True
    except Exception as e:
        print(Fore.RED + f"–û–®–ò–ë–ö–ê –≤–æ –≤—Ä–µ–º—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        send_logs_to_telegram(f"üî¥ –û–®–ò–ë–ö–ê –ê–í–¢–û–†–ò–ó–ê–¶–ò–ò!\n\n–°–∫—Ä–∏–ø—Ç –Ω–µ —Å–º–æ–≥ –≤–æ–π—Ç–∏ –≤ –∞–∫–∫–∞—É–Ω—Ç.\n–û—à–∏–±–∫–∞: {e}")
        return False


def parse_product_page(page: Page) -> dict:
    product_container_selector = "div.itemInfo.group"
    # print(Style.DIM + f"  - –ñ–¥—É –ø–æ—è–≤–ª–µ–Ω–∏—è –≥–ª–∞–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ —Ç–æ–≤–∞—Ä–∞ ('{product_container_selector}')...")
    page.wait_for_selector(product_container_selector, timeout=25000)
    # print(Style.DIM + "  - –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞—é —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö.")
    container = page.locator(product_container_selector)
    ga_data_locator = container.locator(".itemInfoDetails[data-ga-object]")
    if ga_data_locator.count() == 0:
        raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω –æ—Å–Ω–æ–≤–Ω–æ–π JSON-–±–ª–æ–∫ –¥–∞–Ω–Ω—ã—Ö (data-ga-object)")
    ga_data_str = ga_data_locator.get_attribute("data-ga-object")
    ga_data = json.loads(ga_data_str)
    item_info = ga_data["items"][0]
    item_id = item_info.get("item_id")
    brand = item_info.get("item_brand", "N/A")
    price = float(item_info.get("price", 0.0))
    article = f"goods_{item_id}"
    name_selector = "div.ProductHead__name, h1.ProductHead__name"
    name = container.locator(name_selector).first.inner_text().strip()
    description_locator = container.locator(".infoDescription__full")
    description = description_locator.inner_text().strip() if description_locator.count() > 0 else ""
    characteristics = {}
    if container.locator("ul.infoFeatures li.specTitle:has-text('–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏')").count() > 0:
        char_elements = container.locator("ul.infoFeatures li:not(.specTitle)").all()
        for li in char_elements:
            text = li.inner_text().strip()
            parts = re.split(r'\s+[‚Äî:]\s+', text, maxsplit=1)
            if len(parts) == 2:
                characteristics[parts[0].strip()] = parts[1].strip()
    stocks = {}
    stock_rows = page.locator("tr.AvailabilityItem").all()
    for row in stock_rows:
        store_name_locator = row.locator(".AvailabilityLabel")
        if not store_name_locator.count(): continue
        store_name = store_name_locator.inner_text().strip()
        amount_locator = row.locator(".AvailabilityBox--green")
        amount = amount_locator.inner_text().strip() if amount_locator.count() > 0 else "0"
        stocks[store_name] = amount
    image_urls = set()
    image_locators = container.locator(".ProductPhotoThumb__link").all()
    if image_locators:
        for thumb in image_locators:
            href = thumb.get_attribute('href')
            if href and href.startswith('https://s3.ibta.ru'):
                image_urls.add(href)
    else:
        main_image_locator = container.locator(".itemInfoPhotos__link")
        if main_image_locator.count() > 0:
            href = main_image_locator.get_attribute('href')
            if href and href.startswith('https://s3.ibta.ru'):
                image_urls.add(href)
    order_block_selector = container.locator("div.order")
    red_status_locator = order_block_selector.locator(".ProductState--red")
    if red_status_locator.count() > 0:
        status_text = red_status_locator.first.inner_text().strip()
        if "–ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω" in status_text or "–ï—Å—Ç—å —Ç–æ–ª—å–∫–æ –≤ –¥—Ä—É–≥–æ–º —Å–æ—á–µ—Ç–∞–Ω–∏–∏" in status_text:
            raise ValueError(f"–¢–æ–≤–∞—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {status_text}")
        if "–í—ã–≤–µ–¥–µ–Ω" in status_text:
            raise ValueError("–¢–æ–≤–∞—Ä –≤—ã–≤–µ–¥–µ–Ω –∏–∑ –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç–∞")
    return {
        "name": name, "price": price, "brand": brand, "stocks": stocks,
        "description": description, "characteristics": characteristics,
        "image_urls": list(image_urls), "product_url": page.url,
        "article_from_page": article,
        "code": item_id
    }


def main():
    init(autoreset=True)
    start_time = datetime.datetime.now()
    start_message = f"üöÄ –ü–∞—Ä—Å–µ—Ä Officemag (–®–∞–≥ 3) –∑–∞–ø—É—â–µ–Ω –≤ {start_time.strftime('%H:%M:%S')}"
    print(Fore.CYAN + start_message)

    try:
        urls_to_parse = read_simple_urls(INPUT_URL_FILE)
        all_data = load_existing_data(OUTPUT_JSON_FILE)

        def get_article_from_url(url: str) -> str | None:
            match = re.search(r'/goods/(\d+)', url)
            if match: return f"goods_{match.group(1)}"
            return None

        urls_to_process = [url for url in urls_to_parse if
                           (article := get_article_from_url(url)) and article not in all_data]

        if not urls_to_process:
            print(Fore.YELLOW + "–í—Å–µ —Ç–æ–≤–∞—Ä—ã –∏–∑ —Å–ø–∏—Å–∫–∞ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
            send_logs_to_telegram("‚úÖ –í—Å–µ —Ç–æ–≤–∞—Ä—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã. –ù–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–µ—Ç.")
            return

        print(f"–ö –æ–±—Ä–∞–±–æ—Ç–∫–µ {Fore.CYAN}{len(urls_to_process)}{Style.RESET_ALL} –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫.")
        newly_added_count = 0

        with sync_playwright() as p:
            browser = p.chromium.launch(headless=HEADLESS_MODE)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36")
            context.set_default_timeout(TIMEOUT)
            page = context.new_page()

            if not perform_login(page):
                browser.close()
                return

            for url in tqdm(urls_to_process, desc="–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö"):
                product_data = None
                article_id_from_url = get_article_from_url(url)
                if not article_id_from_url:
                    log_failed_url(url, "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL, –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å ID", OUTPUT_FAILED_FILE)
                    continue

                for attempt in range(MAX_RETRIES):
                    try:
                        # print(f"\n  [–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{MAX_RETRIES}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {url}")
                        page.goto(url, wait_until="domcontentloaded")
                        page.evaluate(
                            "() => { const chat = document.querySelector('.online-chat-root-TalkMe'); if (chat) chat.remove(); }")
                        product_data = parse_product_page(page)
                        if product_data:
                            # print(Fore.GREEN + f"  [–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}] –£—Å–ø–µ—à–Ω–æ!")
                            break
                    except Exception as e:
                        print(Fore.RED + f"  [–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}] –û–®–ò–ë–ö–ê: {e}")
                        debug_article_id_with_attempt = f"{article_id_from_url}_attempt_{attempt + 1}"
                        save_debug_info(page, debug_article_id_with_attempt)
                        if attempt < MAX_RETRIES - 1:
                            print(Style.DIM + "  -> –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                            time.sleep(10)

                if product_data:
                    all_data[article_id_from_url] = product_data
                    newly_added_count += 1
                    save_json_data(all_data, OUTPUT_JSON_FILE)
                else:
                    print(Fore.RED + Style.BRIGHT + f"–ù–ï –£–î–ê–õ–û–°–¨ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å {url} –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫.")
                    if not any(url in line for line in
                               (open(OUTPUT_FAILED_FILE).readlines() if os.path.exists(OUTPUT_FAILED_FILE) else [])):
                        log_failed_url(url, "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–ª–∏ —Å–ø–∞—Ä—Å–∏—Ç—å –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫", OUTPUT_FAILED_FILE)

                time.sleep(random.uniform(*PAUSE_BETWEEN_REQUESTS))

            browser.close()

        end_time = datetime.datetime.now()
        duration = end_time - start_time

        finish_message = (
            f"‚úÖ –ü–∞—Ä—Å–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É.\n\n"
            f"üëç –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {newly_added_count}\n"
            f"üíæ –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –±–∞–∑–µ: {len(all_data)}\n"
            f"üïí –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {str(duration).split('.')[0]}"
        )
        print("-" * 50)
        print(Fore.CYAN + finish_message)
        send_logs_to_telegram(finish_message)

    except Exception as e:
        error_message = f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ –ø–∞—Ä—Å–µ—Ä–µ!\n\n–°–∫—Ä–∏–ø—Ç –∞–≤–∞—Ä–∏–π–Ω–æ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è.\n\n–û—à–∏–±–∫–∞:\n{e}"
        print(Fore.RED + Style.BRIGHT + error_message)
        send_logs_to_telegram(error_message)


if __name__ == '__main__':
    main()
